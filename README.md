# Marine Hypoxia Analysis

From the spring through the summer in the Gulf of Mexico, scientists have measured the water having very low oxygen levels in a cycle known as hypoxia. Many species can’t survive at these levels, leaving a dead zone. The Gulf of Mexico is one of the most important regions for commercial fishing in the US, meaning that this issue has major economic and social impacts. One of the biggest reasons behind this dead zone is eutrophication, or the excess of phosphorus and nitrogen causing algal blooms. When they die and decompose, oxygen is consumed from the water, leaving little for other organisms (NOAA). Currently, many models approaching this problem restrict themselves to data about the oceans (Ahmad et al.) or they stick to predicting current conditions or short-term forecasting. By attempting to conduct long-term forecasting, new models could allow for larger scale management decisions and prevention actions. This can help decrease the ecological impact of eutrophication by saving fish or minimizing habitat loss and support commercial farming by giving a larger gap for them to adjust to future hypoxia.

The main data source that has the actual bottom dissolved oxygen levels is the NCEI NOAA Gulf of America Hypoxia Watch website (link). The data has 152 stations across the Gulf of Mexico where measurements were taken (one measurement was taken at each station across early June to mid July). The data has the geographical markers for where the measurements were taken. I can aggregate this data to group different stations together across the gulf and run multiple regression models for each location. Next, to get nutrient information, I will use SNAPD (link), a dataset that is comprised of observations for nitrogen- and phosphorus-containing compounds across the Mississippi/Atchafalaya River Basin over 38 years. The data has a daily granularity, so I can aggregate it to monthly averages to use in my model. The USGS water data API (link) will give me access to historical streamflow data from relevant sites. I can incorporate precipitation and other weather data from NOAA (link). Finally, I can incorporate agricultural practice data for locations along the river. There is fertilizer use (link) and agricultural land use such as types of crops (link) that might be valuable to include in the modeling. This data is annual, however, so incorporating it will be tricky. Also, I hope to maintain the spatial granularity of the different input datasets by including separate farming communities along the Mississippi river as different inputs. By running multiple regression, I can try to predict the oxygen in different regions.

In terms of the actual model, I plan on using an LSTM (DSAN 5600 Time Series Resource, LSTM models). This model incorporates time series data with deep learning models, allowing for complexities in multi-variate data to be captured. The output of the dense output layers could either be a binary classification of whether there will be a hypoxic zone or a regression for the actual dissolved oxygen across a variety of grouped stations in the gulf (or all 152 stations). Other research has used this method to predict water quality over time (Gao et al.) with fairly high success, so it would be interesting to see it applied to this use case with this dataset. The architecture itself will need to be determined using hyperparameter tuning and possibly cross validation. Different models could be made for each region, or the final dense layer could have multiple outputs. Also, there could be comparison between window lead times to see how big of a lag leads to the most accurate model. For example, including data up through May to predict peak levels for July would be a 2-month lead time. I could test this for data through June or March to see if there is an optimal time frame. I could also expand this to be multi-year lead times with moving averages. Maximizing this lead time can lead to earlier predictions, allowing for fisheries enough time to make management decisions, for example.

Finally, in terms of evaluating the model’s performance, I will either use the accuracy/precision/recall/F1-score and a confusion matrix if I decide to do a binary classification problem or R², RMSE, or MAE if this becomes a regression model. I can then compare this to my own random forest model created with just the oceanography features I collected (Ahmad et al.) and other baseline time series models to see if this model performs better. Then, I will include a comparison of lead times to find which models are the strongest and most useful to industry stakeholders. Over the course of training and testing this model, I will measure and report my model’s carbon emission using CodeCarbon to ensure that I am conscious of the environmental impact of this work and am focusing on the overall efficiency of the development process as well as the accuracy of the model.